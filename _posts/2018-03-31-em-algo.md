--- 
layout: post
title:  上帝的算法 —— Expectation–maximization algorithm
tags:
- 机器学习
status: publish
type: post
published: true
---

<br>

![EM-algorithm](https://i.imgur.com/aJQfIYQ.gif)
	
> “Nice intuitions have nice mathematical explanations. Not every iterative procedure can be called EM.”  —— [Xiaojin Zhu](http://pages.cs.wisc.edu/~jerryzhu/cs838/EM.pdf)
	
# 1. 问题是这样子的：
	
![coin question](https://i.imgur.com/G255mev.png)
	
	

Given two coins A and B, repeat the following procedure five times:
	
Randomly choose one of the two coins(with equal probability), perform ten independent coin tosses with the selected coin.
	
Question: What's the probability that coin A and B will land on heads respectively?
	
现有两个硬币A和B，重复一下做法5次：
	
随机从两个硬币中选一个(选中概率相同)，然后独立抛此硬币10次。
	
问：硬币A和B抛到正面的概率各是多少？
	
	
# 2. 一个理想思路是：
	
1) 先化简一下问题，如果只有一个硬币呢？
	
那么：硬币抛正面的概率 = 硬币抛正面的次数 / 硬币抛的总次数 = 0.66 (1)
	
![one coin](https://i.imgur.com/TCgnzCx.png)
	
2) 如果变成两个硬币，会有什么问题呢？
	
- 你需要知道每一次**「取的是哪一个硬币」**，你才能像公式(1)去计算「硬币抛正面的概率」，然而你**并不知道**。
- 那能不能根据抛的结果，来猜这次**「取的是哪一个硬币」**各自的概率呢？用最大似然估计做的话，这又需要知道每个**「硬币抛正面的概率」**。
	
3) **「取的是哪一个硬币」 vs 「硬币抛正面的概率」, 这二者求任何一个，都需要知道对方**，这就形成了一个死锁，想到了启动水泵，要先加水，我们能不能先假定一个「硬币抛正面的概率」，然后通过「迭代」的方式来优化答案呢？
	
4) 如果我们知道P(A)和P(B)了，那么我们就可以利用**MLE(最大似然估计)**去计算每一轮，取到硬币A和硬币B的概率各是多少，然后利用公式(1)去计算，与之前不同的是，这一次我们的「次数」不再是整数，而是小数。原因是：我们不能确定这一轮一定是硬币A或者硬币B，那何不我们各分一部分呢？
	
![硬币A和B](https://i.imgur.com/NzlUlft.png)
	
5) 这样不断迭代，我们得到一个不错的答案，从SSCC思路(Statement-Solution-Correctness-Complexity)出发，我需要证明其正确性？
	
- EM算法能保证收敛么？(可以)
- 如果收敛，保证一定是全局最优解么？（不能，有可能会陷入局部最优解）

# 3. 小结
	
> EM（Expectation-Maximization）算法是常用的估计参数隐变量的利器，它是一种迭代式的方法。在现实应用中，常常会遇到不完整的数据集，比如由于西瓜的根蒂已脱落，无法看出是“蜷缩”还是“硬挺”，则训练样本的“根蒂”属性变量值未知。此时，可以通过EM算法EM算法去解决，EM算法的基本思想是：
> 
-  ① 若参数Θ已知，则可根据训练数据推断出最优隐变量 Z 的值（E步）；
-  ② 若 Z 的值已知，则可方便地对参数Θ做极大似然估计（M步）
>
—— [ 周志华，《机器学习》，P163]
	
后来，Dempster–Laird–Rubin的paper让这个叫做EM算法，大放光彩。值得一提的是，关于EM的收敛性，1977年的Dempster–Laird–Rubin paper中的证明还是错误的，到了1983年， C.F.Jeff Wu才给出了正确的证明。
	
简而言之，EM算法，是MLE的扩展，因为有一些变量无法观察到的(每一次取的是哪一个硬币)，所以，MLE无法做，这时候你就先猜一个答案(parameter)，然后根据这个答案，估计「你无法观察到的」(E-step), 然后再用这个估计，去寻找使得E步能产生最大似然期望最大化的参数值，即更新你的猜测(parameter).
	
规范一点地说，EM算法使用两个步骤交替计算：
	
第一步，期望（E步）：利用当前估计的参数值 Θ(i) 来计算对数似然的期望值（为什么这里要计算的是对数似然函数？因为1、对数似然函数是将原始似然函数的连乘变成了连加；2、通常似然函数中的值都比较小，许多很小的数字相乘起来在计算机里很容易造成浮点数下溢，所以对其取对数将其变成连加的。）；
	
第二步，最大化（M步）：寻找能使E步产生的似然期望最大化的参数值 Θ(i+1)
	
然后，新得到的参数值重新被用于E步，如此循环迭代，直至收敛到局部最优解。
	
# 4. Show me the code 
	
[Solve coin problems using EM algorithm](https://people.duke.edu/~ccc14/sta-663/EMAlgorithm.html)
	
# 5. 再多玩一会
	
1. K-means是一种Hard EM算法，而GMM迭代求解是K-means的典型应用，来细说一下他们之间的关系？来读读[漫谈 Clustering 系列](http://blog.pluskid.org/?page_id=78)
2. EM算法作为聚类算法的核心，通透理解EM对于理解非监督学习很重要，试图来讲讲EM的九层理解？来读读[Hinton和Jordan理解的EM算法](https://www.jianshu.com/p/bfa6b5947cd9)
3. 证明EM算法的收敛性？可参考Andrew Ng的CS229的[讲义](http://cs229.stanford.edu/notes/cs229-notes8.pdf) 和[The Expectation Maximization Algorithm A short tutorial](http://www.seanborman.com/publications/EM_algorithm.pdf)
4. 想了解一下EM算法的各种变体么？来读读[EM algorithm and variants: an informal tutorial](https://arxiv.org/pdf/1105.1476.pdf)
5. 想动手用代码解决一下上面的硬币问题？看看读读[Expectation Maximizatio (EM) Algorithm¶](https://people.duke.edu/~ccc14/sta-663/EMAlgorithm.html)
6. Hard Clustering 和 Soft Clustering 有区别呢？ 看[Clustering 2: soft vs. hard clustering](https://www.youtube.com/watch?v=ThgJzGYVWzc)
7. 梯度下降法和EM算法有什么关系呢？看[梯度下降和EM算法：系出同源，一脉相承](https://kexue.fm/archives/4277)
8. 进一步讲，Gradient descent和coordinate descent有什么区别呢？“就像hill climbing在实数领域就是Gradient descennt一样，min conflicts在实数领域就是coordinate descent.
9. 试着使用EM算法思想去理解SMO算法，和Lasso回归算法? 在E步，我们所做的事情是固定模型参数的值，优化隐含数据的分布，而在M步，我们所做的事情是固定隐含数据分布，优化模型参数的值。

# 6. 感谢
	
	
1. Credit to Master Zhao 
1. [An Evening with… EM!](https://github.com/dirkhovy/emtutorial/blob/master/An%20Evening%20with%20EM.ipynb) :  最大实践新手指南！
1. [Chuong B Do & Serafim Batzoglou: What is the expectation maximization](https://github.com/WillWang-X/machine-learning-visualization/blob/master/Reference/whatisEM.pdf) : EM算法最通俗的例子
1. [EM algorithm: how it works](https://www.youtube.com/watch?v=REypj2sy_5U&index=1&list=PLBv09BD7ez_4e9LtmK626Evn1ion6ynrt) : EM视频讲解最佳，高斯分布。
1. [Priming the pump: a cartoon history](http://languagelog.ldc.upenn.edu/nll/?p=32649) : 感觉EM算法，有点想启动水泵的策略，先放点水，然后才能得到更多的水。 更多可：特朗普想出的“新名词”：启动水泵
1. [Hinton和Jordan理解的EM算法](https://www.jianshu.com/p/bfa6b5947cd9)
1. [A view of the EM algortihm that justifies incremental, sparse, and pther variants](http://www.cs.toronto.edu/~fritz/absps/emk.pdf) : EM算法的变体
1. [Counting clusters with mixture models and EM](https://mollermara.com/blog/em-algorithm/ ) : 视觉化算法， 有代码
1. [GMM and EM algorithm Code](https://github.com/hit-computer/Algorithm/tree/master/EM%20Algorithm) and [Data and Code](https://github.com/ali92hm/expectation-maximization)



<br>
<br>

简之           
2018.03.31          

